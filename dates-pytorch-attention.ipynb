{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial: Sequence-to-Sequence Attention for Date Formats\n",
    "\n",
    "Recently I set upon picking up PyTorch to discover the power of dynamic computational graphs. The notion of being Python-friendly, more readable, and easier to debug applealed to me as those can serve well early in the prototyping and research phases. Part of my learning cycle is to implement problems from different domains - as they greatly help in rapid learning.\n",
    "\n",
    "So, for this I was heavily inspired by an excellent tutorial that Zafaralia Ahmed created in Keras for a neural network model that translates \"human\" dates (for example, \"Nov 5, 2016\") to a standarized machine-format date (for example, \"2016-11-05\"). Since my tutorial below jumps right into code, I would highly recommend reading the Medium post before as it provides a lot of great background on sequence-to-sequence models.\n",
    "\n",
    "[How to Visualize Your Recurrent Neural Network with Attention in Keras](https://medium.com/datalogue/attention-in-keras-1892773a4f22)\n",
    "\n",
    "Of course, since the Medium post outlines the implementation in Keras, those parts while useful to see the implementation details in Keras, will be different from the PyTorch implementation below.\n",
    "\n",
    "Also, I love being able to step through code interactively as it really helps me learn what is going on. Thus I've taken some of the data processing code from the Keras tutorial and provided it in-line to this tutorial so anyone can simply step through the cells. This does assume that any requirements needed have already been installed in your Python environment.\n",
    "\n",
    "Finally as with any learning process, I could not have gotten here without some excellent tutorials and open source code out there. Special thanks to:\n",
    "\n",
    "[Translation with a Sequence to Sequence Network and Attention](http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "[pytorch seq2seq](https://github.com/rowanz/pytorch-seq2seq)\n",
    "\n",
    "Further, if you are new to PyTorch, I would definitely recommend going though the PyTorch tutorials before as they provide great building blocks to get here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, PackedSequence\n",
    "    \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "Here we will create the training & validation data. The creation process generates the data to be used by training. Subsequentaly we will load this data using PyTorch data processing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://github.com/datalogue/keras-attention/tree/master/data\n",
    "\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from faker import Faker\n",
    "import babel\n",
    "from babel.dates import format_date\n",
    "\n",
    "fake = Faker()\n",
    "fake.seed(230517)\n",
    "random.seed(230517)\n",
    "\n",
    "# This is the complete set of formats from original implementation\n",
    "# For the purpose of simplicity, I've selected only one. However, feel\n",
    "# free to add others to see how the training changes with more formats\n",
    "FORMATS_MANY = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY',\n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY',\n",
    "           ]\n",
    "\n",
    "FORMATS = ['long', \"medium\"]\n",
    "\n",
    "# change this if you want it to work with only more than US English\n",
    "# again, for faster learning time for tutorial, it is currently set to US English\n",
    "# LOCALES = babel.localedata.locale_identifiers()\n",
    "LOCALES = ['en_US'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/datalogue/keras-attention/tree/master/data\n",
    "\n",
    "def create_date(change_case=False):\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing \n",
    "                  1. human formatted string\n",
    "                  2. machine formatted string\n",
    "                  3. date object.\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    # wrapping this in a try catch because\n",
    "    # the locale 'vo' and format 'full' will fail\n",
    "    try:\n",
    "        human = format_date(dt,\n",
    "                            format=random.choice(FORMATS),\n",
    "                            locale=random.choice(LOCALES))\n",
    "        \n",
    "        if change_case:\n",
    "            case_change = random.randint(0,3) # 1/2 chance of case change\n",
    "            if case_change == 1:\n",
    "                human = human.upper()\n",
    "            elif case_change == 2:\n",
    "                human = human.lower()\n",
    "\n",
    "        machine = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        # print(e)\n",
    "        return None, None, None\n",
    "\n",
    "    return human, machine, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a sample date\n",
    "human, machine, dt = create_date()\n",
    "print([human, machine, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://github.com/datalogue/keras-attention/tree/master/data\n",
    "\n",
    "def create_dataset(dataset_name, n_examples, vocabulary=False):\n",
    "    \"\"\"\n",
    "        Creates a csv dataset with n_examples and optional vocabulary\n",
    "        :param dataset_name: name of the file to save as\n",
    "        :n_examples: the number of examples to generate\n",
    "        :vocabulary: if true, will also save the vocabulary\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "\n",
    "    with open(dataset_name, 'w') as f:\n",
    "        for i in range(n_examples):\n",
    "            h, m, _ = create_date()\n",
    "            if h is not None:\n",
    "                f.write('\"'+h + '\",\"' + m + '\"\\n')\n",
    "                human_vocab.update(tuple(h))\n",
    "                machine_vocab.update(tuple(m))\n",
    "\n",
    "    if vocabulary:\n",
    "        int2human = dict()\n",
    "        int2human[0] =  '<unk>'\n",
    "        for i,j in enumerate(human_vocab):\n",
    "            int2human[i+1] = j\n",
    "        int2human.update({len(int2human): '<eot>',\n",
    "                          len(int2human)+1: '<bot>'})\n",
    "        int2machine = dict()\n",
    "        int2machine[0] =  '<unk>'\n",
    "        for i,j in enumerate(machine_vocab):\n",
    "            int2machine[i+1] = j\n",
    "        int2machine.update({len(int2machine):'<eot>',\n",
    "                            len(int2machine)+1: '<bot>'})\n",
    "\n",
    "        human2int = {v: k for k, v in int2human.items()}\n",
    "        machine2int = {v: k for k, v in int2machine.items()}\n",
    "\n",
    "        with open('human_vocab.json', 'w') as f:\n",
    "            json.dump(human2int, f)\n",
    "        with open('machine_vocab.json', 'w') as f:\n",
    "            json.dump(machine2int, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('creating dataset')\n",
    "NUM_TRAINING_SAMPLES = 100000\n",
    "NUM_VALIDATION_SAMPLES = 1000\n",
    "create_dataset('training.csv', NUM_TRAINING_SAMPLES, vocabulary=True)\n",
    "create_dataset('validation.csv', NUM_VALIDATION_SAMPLES)\n",
    "print('dataset created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Now we will create a custom dataset class to load the data from the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/datalogue/keras-attention/tree/master/data\n",
    "\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self, vocabulary_file, padding=None):\n",
    "        \"\"\"\n",
    "            Creates a vocabulary from a file\n",
    "            :param vocabulary_file: the path to the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocabulary_file = vocabulary_file\n",
    "        with open(vocabulary_file, 'r') as f:\n",
    "            self.vocabulary = json.load(f)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.reverse_vocabulary = {v: k for k, v in self.vocabulary.items()}\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "            Gets the size of the vocabulary\n",
    "        \"\"\"\n",
    "        return len(self.vocabulary.keys())\n",
    "\n",
    "    def string_to_int(self, text):\n",
    "        \"\"\"\n",
    "            Converts a string into it's character integer \n",
    "            representation\n",
    "            :param text: text to convert\n",
    "        \"\"\"\n",
    "        characters = list(text)\n",
    "\n",
    "        integers = []\n",
    "\n",
    "        if self.padding and len(characters) >= self.padding:\n",
    "            # truncate if too long\n",
    "            characters = characters[:self.padding - 1]\n",
    "\n",
    "        characters.append('<eot>')\n",
    "\n",
    "        for c in characters:\n",
    "            if c in self.vocabulary:\n",
    "                integers.append(self.vocabulary[c])\n",
    "            else:\n",
    "                integers.append(self.vocabulary['<unk>'])\n",
    "\n",
    "\n",
    "        # pad:\n",
    "        if self.padding and len(integers) < self.padding:\n",
    "            integers.extend([self.vocabulary['<unk>']]\n",
    "                            * (self.padding - len(integers)))\n",
    "\n",
    "        if len(integers) != self.padding:\n",
    "            print(text)\n",
    "            raise AttributeError('Length of text was not padding.')\n",
    "        return integers\n",
    "\n",
    "    def int_to_string(self, integers):\n",
    "        \"\"\"\n",
    "            Decodes a list of integers\n",
    "            into it's string representation\n",
    "        \"\"\"\n",
    "        characters = []\n",
    "        for i in integers:\n",
    "            characters.append(self.reverse_vocabulary[i])\n",
    "\n",
    "        return characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data load/transform functions are wrapped in PyTorch Dataset\n",
    "# One of the big benefits for this would be to have a random batch generator\n",
    "# This tutorial currently samples one at a time - so changing the implementation\n",
    "# to handle mini-batches would a great next step.\n",
    "class DateFormatDataset(Dataset):\n",
    "    \"\"\"Date Format dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, input_vocabulary, output_vocabulary):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file human & machine formats.\n",
    "        \"\"\"\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        \n",
    "        self.input_vocab = input_vocabulary\n",
    "        self.output_vocab = output_vocabulary\n",
    "        \n",
    "        self._read_data(csv_file)\n",
    "        \n",
    "        self._transform_data()\n",
    "\n",
    "    def _read_data(self, file_name):\n",
    "        with open(file_name, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                self.inputs.append(row[0])\n",
    "                self.targets.append(row[1])\n",
    "                \n",
    "    def _transform_data(self):\n",
    "        self.inputs = np.array(list(\n",
    "            map(self.input_vocab.string_to_int, self.inputs)))\n",
    "        self.targets = np.array(list(map(self.output_vocab.string_to_int, self.targets)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'input': self.inputs[idx], 'target': self.targets[idx]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=20\n",
    "\n",
    "input_vocab = Vocabulary('human_vocab.json', padding=20)\n",
    "output_vocab = Vocabulary('machine_vocab.json',\n",
    "                              padding=20)\n",
    "\n",
    "training = DateFormatDataset('training.csv', input_vocab, output_vocab)\n",
    "validation = DateFormatDataset('validation.csv', input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab_size=None):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, bidirectional=True)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x)\n",
    "        output, h_n = self.gru(x_embed.view(1, 1, -1))\n",
    "\n",
    "        output_t = output.transpose(0, 1).contiguous()\n",
    "        return output_t, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)))\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            output = F.relu(output)\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "SOS_token = output_vocab.vocabulary[\"<bot>\"]\n",
    "EOS_token = output_vocab.vocabulary[\"<eot>\"]\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size*2))\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei])\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            \n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            if ni == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    plot_accuracy = []\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #criterion = nn.NLLLoss()\n",
    "    for iter in range(1, n_iters+1):\n",
    "        choice = random.randint(0, NUM_TRAINING_SAMPLES-1)\n",
    "        input_variable = torch.LongTensor(training.inputs[choice])\n",
    "        input_variable = Variable(input_variable)\n",
    "        target_variable = torch.LongTensor(training.targets[choice])\n",
    "        target_variable = Variable(target_variable)\n",
    "\n",
    "        loss = train(input_variable, target_variable, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "            # set print_results=True if you would like to see how the predictions are\n",
    "            # comparing to the targets during the training cycle\n",
    "            # I found this part very illuminating as you can see the neural models\n",
    "            # start to learn parts of the pattern over time\n",
    "            accuracy = evaluateRandomly(encoder, decoder, 10, print_results=False)\n",
    "            plot_accuracy.append(accuracy)\n",
    "            print(\"Loss: %0.2f, Accuracy: %0.2f\" % (loss, accuracy))\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    showPlot(plot_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_variable, max_length=MAX_LENGTH):\n",
    "    input_length = input_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size*2))\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[ei])\n",
    "        encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]]))  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_output, encoder_outputs)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<eot>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_vocab.reverse_vocabulary[ni])\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n, print_results=False):\n",
    "    total_score = 0\n",
    "    for i in range(n):\n",
    "        choice = random.randint(0, NUM_VALIDATION_SAMPLES-1)\n",
    "        input_variable = torch.LongTensor(validation.inputs[choice])\n",
    "        input_variable = Variable(input_variable)\n",
    "        target_variable = torch.LongTensor(validation.targets[choice])\n",
    "        target_variable = Variable(target_variable)\n",
    "        \n",
    "        target_sentence = output_vocab.int_to_string(target_variable.data)\n",
    "        if print_results:\n",
    "            print('>', input_vocab.int_to_string(input_variable.data))\n",
    "            print('=', target_sentence)\n",
    "        output_words, attentions = evaluate(encoder, decoder, input_variable)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        if print_results:\n",
    "            print('<', output_sentence)\n",
    "            \n",
    "        # determine the accuracy score\n",
    "        # here it measures % of input characters were correctly matched\n",
    "        score = 0\n",
    "        for di, letter in enumerate(output_words):\n",
    "            if letter == target_sentence[di]:\n",
    "                score += 1\n",
    "        \n",
    "        total_score += (score / len(output_words))\n",
    "        \n",
    "        if print_results:\n",
    "            print('')\n",
    "        \n",
    "    return (total_score / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 256\n",
    "NUM_ITERATIONS = 20000\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    MAX_LENGTH,\n",
    "    HIDDEN_SIZE,\n",
    "    vocab_size=input_vocab.size()\n",
    ")\n",
    "\n",
    "attn_decoder = AttnDecoderRNN(HIDDEN_SIZE, output_vocab.size(),\n",
    "                               1, dropout_p=0.1)\n",
    "\n",
    "trainIters(encoder, attn_decoder, NUM_ITERATIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
